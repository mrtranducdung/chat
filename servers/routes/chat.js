import { Router } from 'express';
import { optionalAuth } from '../middleware/auth.js';
import { db } from '../config/database.js';
import { cosineSimilarity } from '../utils/rags.js';
import { Mistral } from '@mistralai/mistralai';
import { MISTRAL_API_KEY } from '../config/env.js';

const router = Router();
const mistral = new Mistral({ apiKey: MISTRAL_API_KEY });

// ============================================
// CHAT ENDPOINT WITH RAG
// ============================================

router.post('/', optionalAuth, async (req, res) => {
  try {
    const { message, history, botName, language, tenantId: bodyTenantId } = req.body;
    
    if (!MISTRAL_API_KEY) {
      return res.status(503).json({ error: 'AI service not configured' });
    }

    const tenantId = req.user?.tenantId || bodyTenantId || 'default';

    console.log(`ğŸ’¬ Chat request - Tenant: ${tenantId}, Language: ${language}`);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // RAG: Search for relevant chunks
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    let contextText = '';
    let ragUsed = false; // âœ… Track RAG usage
    let ragChunksCount = 0; // âœ… Track number of chunks used
    let ragSimilarity = 0; // âœ… Track max similarity
    
    try {
      // Generate embedding for user query
      const queryEmbeddingResponse = await mistral.embeddings.create({
        model: 'mistral-embed',
        inputs: [message]
      });
      const queryEmbedding = queryEmbeddingResponse.data[0].embedding;

      // Get all chunks for tenant
      const chunks = db.prepare(`
        SELECT id, chunk_text, embedding 
        FROM document_chunks 
        WHERE tenant_id = ?
      `).all(tenantId);

      if (chunks.length > 0) {
        console.log(`ğŸ” RAG: Searching ${chunks.length} chunks...`);

        // Calculate similarity for each chunk
        const results = chunks.map(chunk => {
          const chunkEmbedding = JSON.parse(chunk.embedding);
          const similarity = cosineSimilarity(queryEmbedding, chunkEmbedding);
          return { text: chunk.chunk_text, similarity };
        });

        // Get top 3 most relevant chunks
        results.sort((a, b) => b.similarity - a.similarity);
        const topChunks = results.slice(0, 3);

        // Only use chunks with similarity > 0.3 (30%)
        if (topChunks[0].similarity > 0.3) {
          contextText = topChunks.map((c, i) => `[Chunk ${i+1}] ${c.text}`).join('\n\n');
          ragUsed = true; // âœ… Mark as used
          ragChunksCount = topChunks.length;
          ragSimilarity = topChunks[0].similarity;
          console.log(`âœ… RAG: Found ${topChunks.length} relevant chunks (similarity: ${topChunks[0].similarity.toFixed(3)})`);
        } else {
          console.log(`âš ï¸  RAG: No highly relevant chunks (max similarity: ${topChunks[0].similarity.toFixed(3)})`);
        }
      } else {
        console.log('ğŸ“­ RAG: No chunks available');
      }
    } catch (ragError) {
      console.error('RAG search failed, continuing without context:', ragError);
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Build system prompt with RAG context
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    const systemInstruction = language === 'vi'
      ? `Báº¡n lÃ  ${botName || 'trá»£ lÃ½ AI'}. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, ngáº¯n gá»n vÃ  chuyÃªn nghiá»‡p.

${contextText ? `NGá»® Cáº¢NH TÃ€I LIá»†U:
${contextText}

Náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n tÃ i liá»‡u trÃªn, hÃ£y sá»­ dá»¥ng thÃ´ng tin Ä‘Ã³ Ä‘á»ƒ tráº£ lá»i.` : 'KhÃ´ng cÃ³ tÃ i liá»‡u nÃ o.'}

Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong tÃ i liá»‡u, tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c chung vÃ  nÃ³i rÃµ lÃ  báº¡n khÃ´ng cháº¯c cháº¯n.`
      : `You are ${botName || 'an AI assistant'}. Answer in English, concisely and professionally.

${contextText ? `DOCUMENT CONTEXT:
${contextText}

If the question relates to the documents above, use that information to answer.` : 'No documents available.'}

If information is not found in documents, answer based on general knowledge and mention uncertainty.`;

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Set headers (including RAG metadata)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    // âœ… Set RAG headers BEFORE streaming starts
    res.setHeader('X-RAG-Used', ragUsed ? 'true' : 'false');
    res.setHeader('X-RAG-Chunks', ragChunksCount.toString());
    res.setHeader('X-RAG-Similarity', ragSimilarity.toFixed(3));
    
    // Standard streaming headers
    res.setHeader('Content-Type', 'text/plain; charset=utf-8');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('Transfer-Encoding', 'chunked');

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Build messages array for Mistral
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    const messages = [
      { role: 'system', content: systemInstruction },
      ...history.filter(m => m.id !== 'welcome').map(msg => ({
        role: msg.sender === 'user' ? 'user' : 'assistant',
        content: msg.text
      })),
      { role: 'user', content: message }
    ];

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stream response from Mistral
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.log(`ğŸ¤– Sending to Mistral AI...`);

    const chatStream = await mistral.chat.stream({
      model: 'mistral-large-latest',
      messages: messages,
    });

    let totalChunks = 0;
    for await (const chunk of chatStream) {
      const content = chunk.data.choices[0]?.delta?.content;
      if (content) {
        res.write(content);
        totalChunks++;
      }
    }
    
    res.end();
    console.log(`âœ… Chat response completed (${totalChunks} chunks streamed)`);

  } catch (error) {
    console.error('âŒ Chat error:', error);
    
    // If headers not sent yet, send error JSON
    if (!res.headersSent) {
      res.status(500).json({ 
        error: 'Failed to process message', 
        details: error.message 
      });
    } else {
      // If streaming already started, just end it
      res.end();
    }
  }
});

export default router;